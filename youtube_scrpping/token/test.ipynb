{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.46437994722955145\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63       176\n",
      "           1       0.00      0.00      0.00       203\n",
      "\n",
      "    accuracy                           0.46       379\n",
      "   macro avg       0.23      0.50      0.32       379\n",
      "weighted avg       0.22      0.46      0.29       379\n",
      "\n",
      "Random Forest Accuracy: 0.503957783641161\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.52      0.49       176\n",
      "           1       0.54      0.49      0.51       203\n",
      "\n",
      "    accuracy                           0.50       379\n",
      "   macro avg       0.51      0.51      0.50       379\n",
      "weighted avg       0.51      0.50      0.50       379\n",
      "\n",
      "XGBoost Accuracy: 0.48021108179419525\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.88      0.61       176\n",
      "           1       0.56      0.13      0.22       203\n",
      "\n",
      "    accuracy                           0.48       379\n",
      "   macro avg       0.52      0.51      0.41       379\n",
      "weighted avg       0.52      0.48      0.40       379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download spaCy French model\n",
    "spacy.cli.download(\"fr_core_news_sm\")\n",
    "\n",
    "# Load spaCy French model\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'projetintegrer.csv'  # Update the path if necessary\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize French stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Function to clean tokenized text\n",
    "def clean_text(tokens):\n",
    "    # Remove punctuation and special characters\n",
    "    tokens = [re.sub(r'\\W+', '', token) for token in tokens]\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize using spaCy\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    # Remove short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Apply cleaning function to tokenized text\n",
    "data['Cleaned_Tokens'] = data['Tokenized'].apply(lambda x: clean_text(eval(x)))\n",
    "\n",
    "# Flatten the list of tokens to create a single list of all words\n",
    "all_words = [word for tokens in data['Cleaned_Tokens'] for word in tokens]\n",
    "\n",
    "# Calculate the frequency of each word\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Define a threshold for rare words (e.g., words that appear less than 5 times)\n",
    "threshold = 5\n",
    "\n",
    "# Create a set of rare words\n",
    "rare_words = {word for word, freq in word_freq.items() if freq < threshold}\n",
    "\n",
    "# Function to remove rare words from tokenized text\n",
    "def remove_rare_words(tokens):\n",
    "    return [token for token in tokens if token not in rare_words]\n",
    "\n",
    "# Apply the function to remove rare words\n",
    "data['Cleaned_Tokens'] = data['Cleaned_Tokens'].apply(remove_rare_words)\n",
    "data['Cleaned_Text'] = data['Cleaned_Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Generate dummy sentiment labels for demonstration (replace with actual labels if available)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "data['Sentiment'] = np.random.choice([0, 1], size=len(data))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Cleaned_Text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Adjusting the TF-IDF vectorizer parameters\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Consider only the top 5000 features\n",
    "    ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
    "    max_df=0.95,  # Ignore terms that appear in more than 95% of the documents\n",
    "    min_df=2  # Ignore terms that appear in fewer than 2 documents\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Define hyperparameters for Grid Search\n",
    "param_grid_nb = {\n",
    "    'alpha': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "grid_search_nb = GridSearchCV(nb_model, param_grid_nb, cv=5, scoring='accuracy')\n",
    "grid_search_nb.fit(X_train_tfidf, y_train)\n",
    "nb_best = grid_search_nb.best_estimator_\n",
    "\n",
    "# Train and evaluate the best Naive Bayes model\n",
    "nb_pred = nb_best.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "nb_report = classification_report(y_test, nb_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)\n",
    "\n",
    "# Perform Grid Search for Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_tfidf, y_train)\n",
    "rf_best = grid_search_rf.best_estimator_\n",
    "\n",
    "# Train and evaluate the best Random Forest model\n",
    "rf_pred = rf_best.predict(X_test_tfidf)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_report = classification_report(y_test, rf_pred)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)\n",
    "\n",
    "# Perform Grid Search for XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='accuracy')\n",
    "grid_search_xgb.fit(X_train_tfidf, y_train)\n",
    "xgb_best = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Train and evaluate the best XGBoost model\n",
    "xgb_pred = xgb_best.predict(X_test_tfidf)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_report = classification_report(y_test, xgb_pred)\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Classification Report:\\n\", xgb_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smote in c:\\python312\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\python312\\lib\\site-packages (from smote) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\python312\\lib\\site-packages (from smote) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.19.1->smote) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.19.1->smote) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=0.19.1->smote) (3.2.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\python312\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python312\\lib\\site-packages (from imbalanced-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python312\\lib\\site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\python312\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python312\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python312\\lib\\site-packages (from imbalanced-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement GridSearchCV (from versions: none)\n",
      "ERROR: No matching distribution found for GridSearchCV\n"
     ]
    }
   ],
   "source": [
    "!pip install smote\n",
    "!pip install imbalanced-learn\n",
    "!pip install GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.49340369393139843\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.56      0.51       176\n",
      "           1       0.53      0.44      0.48       203\n",
      "\n",
      "    accuracy                           0.49       379\n",
      "   macro avg       0.50      0.50      0.49       379\n",
      "weighted avg       0.50      0.49      0.49       379\n",
      "\n",
      "Random Forest Accuracy: 0.525065963060686\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.47      0.48       176\n",
      "           1       0.56      0.57      0.56       203\n",
      "\n",
      "    accuracy                           0.53       379\n",
      "   macro avg       0.52      0.52      0.52       379\n",
      "weighted avg       0.52      0.53      0.52       379\n",
      "\n",
      "XGBoost Accuracy: 0.5408970976253298\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.40      0.45       176\n",
      "           1       0.56      0.67      0.61       203\n",
      "\n",
      "    accuracy                           0.54       379\n",
      "   macro avg       0.53      0.53      0.53       379\n",
      "weighted avg       0.54      0.54      0.53       379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "grid_search_nb = GridSearchCV(nb_model, param_grid_nb, cv=5, scoring='accuracy')\n",
    "grid_search_nb.fit(X_train_resampled, y_train_resampled)\n",
    "nb_best = grid_search_nb.best_estimator_\n",
    "\n",
    "# Train and evaluate the best Naive Bayes model\n",
    "nb_pred = nb_best.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "nb_report = classification_report(y_test, nb_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "rf_best = grid_search_rf.best_estimator_\n",
    "\n",
    "# Train and evaluate the best Random Forest model\n",
    "rf_pred = rf_best.predict(X_test_tfidf)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "rf_report = classification_report(y_test, rf_pred)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='accuracy')\n",
    "grid_search_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "xgb_best = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Train and evaluate the best XGBoost model\n",
    "xgb_pred = xgb_best.predict(X_test_tfidf)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_report = classification_report(y_test, xgb_pred)\n",
    "\n",
    "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
    "print(\"XGBoost Classification Report:\\n\", xgb_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted tokens have been saved to newp.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file_path = 'projetintegrer.csv'\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "if 'Cleaned_Tokens' not in df.columns:\n",
    "    raise ValueError(\"mkynach had l column\")\n",
    "\n",
    "tokens = df['Cleaned_Tokens']\n",
    "\n",
    "output_file_path = 'newp.csv'\n",
    "tokens.to_csv(output_file_path, index = False, header = True)\n",
    "\n",
    "print(f\"Extracted tokens have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
