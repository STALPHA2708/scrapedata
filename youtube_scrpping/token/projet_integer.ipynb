{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYKORtCImkID",
        "outputId": "9b5f684d-4443-43ba-da5c-cee40b52e640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in c:\\python312\\lib\\site-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from xgboost) (1.26.2)\n",
            "Requirement already satisfied: scipy in c:\\python312\\lib\\site-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: spacy in c:\\python312\\lib\\site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\python312\\lib\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python312\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\python312\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\python312\\lib\\site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\python312\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python312\\lib\\site-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python312\\lib\\site-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from spacy) (69.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python312\\lib\\site-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\python312\\lib\\site-packages (from spacy) (1.26.2)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python312\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n",
            "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2024.4.16)\n",
            "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement GridSearchCV (from versions: none)\n",
            "ERROR: No matching distribution found for GridSearchCV\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!pip install GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eutbTx_Lmzxc",
        "outputId": "b9757ab0-9e3f-4355-b923-1d759998113b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Qq8oHcRIm9TS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "                                              Tokenized  \\\n",
            "0                             ['lord', 'god', 'angels']   \n",
            "1     ['blessings', 'iv', 'hated', 'needles', 'young...   \n",
            "2     ['stage', 'small', 'cell', 'stage', 'years', '...   \n",
            "3                                             ['delia']   \n",
            "4                               ['get', 'well', 'soon']   \n",
            "...                                                 ...   \n",
            "1886      ['stay', 'strong', 'queen', 'praying', 'fir']   \n",
            "1887  ['sending', 'prayers', 'may', 'god', 'give', '...   \n",
            "1888                     ['hey', 'beautiful', 'missed']   \n",
            "1889                           ['stay', 'strong', 'xx']   \n",
            "1890                         ['stay', 'strong', 'love']   \n",
            "\n",
            "                                         Cleaned_Tokens  \\\n",
            "0                                    [lord, god, angel]   \n",
            "1     [blessing, hated, needle, young, strong, forty...   \n",
            "2     [stage, small, cell, stage, year, took, chemo,...   \n",
            "3                                               [delia]   \n",
            "4                                     [get, well, soon]   \n",
            "...                                                 ...   \n",
            "1886                [stay, strong, queen, praying, fir]   \n",
            "1887  [sending, prayers, may, god, giv, guy, strengt...   \n",
            "1888                           [hey, beautiful, missed]   \n",
            "1889                                     [stay, strong]   \n",
            "1890                              [stay, strong, lover]   \n",
            "\n",
            "                                           Cleaned_Text  \n",
            "0                                        lord god angel  \n",
            "1     blessing hated needle young strong fortys best...  \n",
            "2     stage small cell stage year took chemo immune ...  \n",
            "3                                                 delia  \n",
            "4                                         get well soon  \n",
            "...                                                 ...  \n",
            "1886                      stay strong queen praying fir  \n",
            "1887  sending prayers may god giv guy strength lover...  \n",
            "1888                               hey beautiful missed  \n",
            "1889                                        stay strong  \n",
            "1890                                  stay strong lover  \n",
            "\n",
            "[1891 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download spaCy French model (run this command in your terminal)\n",
        "spacy.cli.download(\"fr_core_news_sm\")\n",
        "\n",
        "# Load spaCy French model\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'projetintegrer.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Initialize French stopwords\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "# Function to clean tokenized text\n",
        "def clean_text(tokens):\n",
        "    # Remove punctuation and special characters\n",
        "    tokens = [re.sub(r'\\W+', '', token) for token in tokens]\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Lemmatize using spaCy\n",
        "    doc = nlp(' '.join(tokens))\n",
        "    tokens = [token.lemma_ for token in doc]\n",
        "    # Remove short words\n",
        "    tokens = [token for token in tokens if len(token) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Apply cleaning function to tokenized text\n",
        "data['Cleaned_Tokens'] = data['Tokenized'].apply(lambda x: clean_text(eval(x)))\n",
        "\n",
        "# Flatten the list of tokens to create a single list of all words\n",
        "all_words = [word for tokens in data['Cleaned_Tokens'] for word in tokens]\n",
        "\n",
        "# Calculate the frequency of each word\n",
        "word_freq = Counter(all_words)\n",
        "\n",
        "# Define a threshold for rare words (e.g., words that appear less than 5 times)\n",
        "threshold = 5\n",
        "\n",
        "# Create a set of rare words\n",
        "rare_words = {word for word, freq in word_freq.items() if freq < threshold}\n",
        "\n",
        "# Function to remove rare words from tokenized text\n",
        "def remove_rare_words(tokens):\n",
        "    return [token for token in tokens if token not in rare_words]\n",
        "\n",
        "\n",
        "# Apply cleaning function to tokenized text\n",
        "data['Cleaned_Tokens'] = data['Tokenized'].apply(lambda x: clean_text(eval(x)))\n",
        "data['Cleaned_Text'] = data['Cleaned_Tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "\n",
        "# Display the cleaned data\n",
        "print(data[['Tokenized', 'Cleaned_Tokens', 'Cleaned_Text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dummy sentiment labels for demonstration\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "data['Sentiment'] = np.random.choice([0, 1], size=len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Cleaned_Text'], data['Sentiment'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy: 0.45910290237467016\n",
            "Naive Bayes Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.45      0.68      0.54       176\n",
            "           1       0.49      0.27      0.35       203\n",
            "\n",
            "    accuracy                           0.46       379\n",
            "   macro avg       0.47      0.47      0.44       379\n",
            "weighted avg       0.47      0.46      0.44       379\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the best Naive Bayes model\n",
        "# Train and evaluate Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "nb_pred = nb_model.predict(X_test_tfidf)\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "nb_report = classification_report(y_test, nb_pred)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
        "print(\"Naive Bayes Classification Report:\\n\", nb_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.5303430079155673\n",
            "Random Forest Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.50      0.50       176\n",
            "           1       0.56      0.56      0.56       203\n",
            "\n",
            "    accuracy                           0.53       379\n",
            "   macro avg       0.53      0.53      0.53       379\n",
            "weighted avg       0.53      0.53      0.53       379\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "rf_pred = rf_model.predict(X_test_tfidf)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "rf_report = classification_report(y_test, rf_pred)\n",
        "\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
        "print(\"Random Forest Classification Report:\\n\", rf_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Accuracy: 0.5118733509234829\n",
            "XGBoost Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.45      0.46       176\n",
            "           1       0.54      0.57      0.55       203\n",
            "\n",
            "    accuracy                           0.51       379\n",
            "   macro avg       0.51      0.51      0.51       379\n",
            "weighted avg       0.51      0.51      0.51       379\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate XGBoost model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_model.fit(X_train_tfidf, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test_tfidf)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
        "xgb_report = classification_report(y_test, xgb_pred)\n",
        "\n",
        "print(\"XGBoost Accuracy:\", xgb_accuracy)\n",
        "print(\"XGBoost Classification Report:\\n\", xgb_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
